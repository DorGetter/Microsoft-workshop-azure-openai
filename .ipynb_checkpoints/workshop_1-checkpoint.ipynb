{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 109,
   "id": "2c28e4dd-2aac-44aa-bcba-2608280db1db",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "from IPython.display import JSON"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "99de53a9-a2b9-4376-8540-720412cebdf5",
   "metadata": {},
   "source": [
    "# Workshop 1: Introduction to OpenAI Chat Completion SDK\n",
    "\n",
    "Welcome to this comprehensive guide on using the OpenAI Chat Completion SDK! In this notebook, we'll dive into the functionalities and features of the SDK, learning how to create engaging and effective conversational AI models. By the end of this notebook, you will be equipped to:\n",
    "\n",
    "1. Utilize the basic chat completion functionality.\n",
    "2. Understand and interpret the request and response structures.\n",
    "3. Configure key parameters such as `temperature`, `max_tokens`, and `n` to fine-tune the model's behavior.\n",
    "4. Implement memory features to enhance the context and coherence of interactions.\n",
    "\n",
    "## Table of Contents\n",
    "\n",
    "1. [Introduction to OpenAI Chat Completion SDK](#introduction-to-openai-chat-completion-sdk)\n",
    "2. [Building Simple Chat Completion Using Python SDK](#building-simple-chat-completion-using-python-sdk)\n",
    "3. [Understanding Request and Response](#understanding-request-and-response)\n",
    "4. [Exploring the Temperature Parameter](#exploring-the-temperature-parameter)\n",
    "5. [Configuring the Max Tokens Parameter](#configuring-the-max-tokens-parameter)\n",
    "6. [Utilizing the N Parameter](#utilizing-the-n-parameter)\n",
    "7. [Enhancing Chat Interactions with Memory](#enhancing-chat-interactions-with-memory)\n",
    "\n",
    "Let's begin by importing the necessary libraries and setting up our OpenAI client.\n",
    "\n",
    "\n",
    "<!--\n",
    "### Additional Context: Understanding the Basics\n",
    "\n",
    "The OpenAI Chat Completion API is designed to generate human-like text based on the input it receives. This is particularly useful for creating chatbots, virtual assistants, and other conversational agents. Here’s a quick breakdown of the key components involved:\n",
    "\n",
    "- **Model**: The specific AI model used for generating the response.\n",
    "- **Messages**: A list of messages that form the conversation. Each message has a role (user, assistant, system) and content.\n",
    "- **API Key**: Your unique key to authenticate and access OpenAI services.\n",
    "\n",
    "Understanding these basics will help you better utilize the API and customize your applications to meet specific needs.\n",
    "-->\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa67ced4-d2b1-4421-95f8-53f19822c202",
   "metadata": {},
   "source": [
    "<!--\n",
    "### Additional Context: Understanding the Basics\n",
    "\n",
    "The OpenAI Chat Completion API is designed to generate human-like text based on the input it receives. This is particularly useful for creating chatbots, virtual assistants, and other conversational agents. Here’s a quick breakdown of the key components involved:\n",
    "\n",
    "- **Model**: The specific AI model used for generating the response.\n",
    "- **Messages**: A list of messages that form the conversation. Each message has a role (user, assistant, system) and content.\n",
    "- **API Key**: Your unique key to authenticate and access OpenAI services.\n",
    "\n",
    "Understanding these basics will help you better utilize the API and cust"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "id": "6eec5eaa-e1dc-4528-8024-2f8fc52260f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "from openai import AzureOpenAI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "id": "162f5aa7-bac2-4c6b-88f5-b6f6c2c11703",
   "metadata": {},
   "outputs": [],
   "source": [
    "client = AzureOpenAI(\n",
    "    azure_endpoint=\"https://testopenai4.openai.azure.com/\",\n",
    "    api_key=\"19f61bb2a418484dbe809cb720c2527b\",\n",
    "    api_version=\"2024-02-15-preview\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "19d22f2a-4c8a-4a9a-9a8b-203a010d8ff7",
   "metadata": {},
   "source": [
    "# Building Simple Chat Completion Using Python SDK\n",
    "\n",
    "To get started, we will demonstrate a basic example of how to use the chat completion API. In this example, we will send a simple message to the model and print the response. This will help you understand the fundamental steps involved in creating a chat completion request.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a77b275-c24a-432b-9c75-6d646287905a",
   "metadata": {},
   "source": [
    "## Basic Request & Response from ChatCompletion\n",
    "\n",
    "The OpenAI Chat Completion API works by exchanging messages between different roles in a conversation. Each message has a specific role, which helps the model understand the context and generate appropriate responses. There are three primary roles:\n",
    "\n",
    "1. **User**: This role represents the person interacting with the model. Messages from the user typically contain questions, prompts, or instructions. For example:\n",
    "   - \"What's the weather like today?\"\n",
    "   - \"Tell me a joke.\"\n",
    "   - \"Explain the theory of relativity.\"\n",
    "\n",
    "2. **Assistant**: This role represents the model's responses. The assistant generates replies based on the user's messages and the conversation history. For example:\n",
    "   - \"The weather today is sunny with a high of 75 degrees.\"\n",
    "   - \"Why don't scientists trust atoms? Because they make up everything!\"\n",
    "   - \"The theory of relativity is a scientific concept developed by Albert Einstein...\"\n",
    "\n",
    "3. **System**: This role provides initial instructions to the model, setting the context or behavior for the conversation. The system message can define the assistant's persona, behavior, or other contextual information. For example:\n",
    "   - \"You are a helpful assistant.\"\n",
    "   - \"You are a customer support agent for a tech company.\"\n",
    "   - \"You are an expert in quantum physics.\"\n",
    "\n",
    "### Example\n",
    "\n",
    "In the following example, we will construct a basic request with messages from these roles and print the response from the model.\n",
    "> **Hint**: Understanding the roles of messages is crucial for designing effective conversational AI models. Here are some additional tips:\n",
    ">\n",
    "> - **User Role**: Craft your user messages to be clear and concise. This helps the model generate better responses. Avoid ambiguous prompts to reduce misunderstandings.\n",
    "> - **Assistant Role**: The assistant's replies should be informative and contextually relevant. Maintaining a consistent tone helps build a coherent conversational flow.\n",
    "> - **System Role**: Use the system role to guide the assistant's behavior and style. This is particularly useful for creating specialized bots (e.g., customer support, technical advisor).\n",
    ">\n",
    "> Experiment with different system messages to see how they influence the assistant's responses.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "id": "bd19b83e-ad30-4276-9048-3df84ff7bf41",
   "metadata": {},
   "outputs": [],
   "source": [
    "system_message = \"You are a helpful assistant.\"\n",
    "input_text = \"write me a short 4 lines poem about smurfes\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "id": "2aeb26b5-874f-436e-9634-f7682c58bf1e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "assistant : In a village so tiny and blue,\n",
      "Smurfs with hearts so true,\n",
      "Laughter and joy they unfurl,\n",
      "In their magical Smurf world.\n"
     ]
    }
   ],
   "source": [
    "# lets run it a few times...\n",
    "response = client.chat.completions.create(\n",
    "    model = \"gpt-4-32k\",\n",
    "    messages = [{\"role\": \"system\", \"content\": system_message},\n",
    "                {\"role\": \"user\", \"content\":input_text}]\n",
    ")\n",
    "print(response.choices[0].message.role, \":\",response.choices[0].message.content)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e547135a-a6aa-4505-9f6c-8e305adc0ff1",
   "metadata": {},
   "source": [
    "## Understanding the Chat Completion Response\n",
    "\n",
    "When you make a request to the OpenAI Chat Completion API, the response contains several key components that provide information about the generated completion and its context. Here’s a breakdown of what you can expect in the response:\n",
    "\n",
    "1. **ID**: A unique identifier for the completion request.\n",
    "2. **Object**: Indicates the type of object returned (e.g., `chat.completion`).\n",
    "3. **Created**: A timestamp indicating when the request was created.\n",
    "4. **Model**: Specifies the model used to generate the completion (e.g., `gpt-4-32k`).\n",
    "5. **Choices**: An array of completion options generated by the model. Each choice contains:\n",
    "   - **Message**: The generated message, which includes:\n",
    "     - **Role**: The role of the message (e.g., `assistant`).\n",
    "     - **Content**: The text content of the message.\n",
    "   - **Finish Reason**: Indicates why the completion ended (e.g., `stop`, `length`).\n",
    "6. **Usage**: Provides token usage statistics, including:\n",
    "   - **Prompt Tokens**: Number of tokens in the input prompt.\n",
    "   - **Completion Tokens**: Number of tokens in the generated completion.\n",
    "   - **Total Tokens**: Total number of tokens used for the request.\n",
    "\n",
    "### Key Points\n",
    "\n",
    "- **ID and Timestamp**: These help you track and manage individual requests.\n",
    "- **Model Information**: Useful for understanding the capabilities and limitations of the response.\n",
    "- **Choices Array**: You can receive multiple completions if you specify the `n` parameter, allowing you to choose the best response.\n",
    "- **Finish Reason**: Helps you understand if the response was cut off due to token limits or other reasons.\n",
    "- **Usage Statistics**: Important for monitoring and managing token consumption, especially in a paid API plan.\n",
    "\n",
    "Understanding these components will help you better interpret the responses from the Chat Completion API and optimize your requests for various applications.\n",
    "\n",
    "> **Hint**: Monitoring the `finish_reason` can help you fine-tune your prompts to avoid incomplete responses. If you frequently encounter the `length` finish reason, consider increasing the `max_tokens` parameter or refining your prompts to be more concise.\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "id": "8952154c-34ca-4191-b2f4-6f55568d5fb7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/json": {
       "choices": [
        {
         "content_filter_results": {
          "hate": {
           "filtered": false,
           "severity": "safe"
          },
          "self_harm": {
           "filtered": false,
           "severity": "safe"
          },
          "sexual": {
           "filtered": false,
           "severity": "safe"
          },
          "violence": {
           "filtered": false,
           "severity": "safe"
          }
         },
         "finish_reason": "stop",
         "index": 0,
         "logprobs": null,
         "message": {
          "content": "In a village so tiny and blue,\nSmurfs with hearts so true,\nLaughter and joy they unfurl,\nIn their magical Smurf world.",
          "function_call": null,
          "role": "assistant",
          "tool_calls": null
         }
        }
       ],
       "created": 1718669757,
       "id": "chatcmpl-9bGntoZK3ikhjd2DPYUp6WbWC26xB",
       "model": "gpt-4-32k",
       "object": "chat.completion",
       "prompt_filter_results": [
        {
         "content_filter_results": {
          "hate": {
           "filtered": false,
           "severity": "safe"
          },
          "self_harm": {
           "filtered": false,
           "severity": "safe"
          },
          "sexual": {
           "filtered": false,
           "severity": "safe"
          },
          "violence": {
           "filtered": false,
           "severity": "safe"
          }
         },
         "prompt_index": 0
        }
       ],
       "system_fingerprint": null,
       "usage": {
        "completion_tokens": 31,
        "prompt_tokens": 29,
        "total_tokens": 60
       }
      },
      "text/plain": [
       "<IPython.core.display.JSON object>"
      ]
     },
     "execution_count": 149,
     "metadata": {
      "application/json": {
       "expanded": false,
       "root": "root"
      }
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "JSON(json.loads(response.model_dump_json()))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "229f8bef-558e-4026-a69c-4c34a97c119f",
   "metadata": {},
   "source": [
    "note: we can see that each run we will get different answer. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "45feb4c4-22d9-47d7-8f4f-4f499ceb3944",
   "metadata": {},
   "source": [
    "### Using the Temperature Hyper-Parameter\n",
    "\n",
    "The `temperature` parameter in the OpenAI Chat Completion API controls the randomness and creativity of the generated text. Adjusting the temperature influences how deterministic or varied the model's responses are.\n",
    "\n",
    "- **What is the Temperature Parameter?**\n",
    "  The temperature parameter adjusts the probability distribution of the next word in the sequence. Lower values (close to 0) make the model more deterministic, producing predictable and focused outputs. Higher values (closer to 1) increase randomness, allowing for more creative and diverse responses.\n",
    "\n",
    "- **How Can We Utilize It?**\n",
    "  - **Low Temperature (e.g., 0.2)**: Ideal for tasks requiring precise and reliable outputs, like factual information retrieval or step-by-step instructions. It reduces variability and creative deviations.\n",
    "  - **High Temperature (e.g., 0.8)**: Suitable for creative tasks such as storytelling or brainstorming, where diverse and imaginative responses are desired.\n",
    "\n",
    "> **Hint**: For more detailed information on the temperature parameter and its effects, you can read further in the [OpenAI Developer Forum](https://community.openai.com/t/temperature-top-p-and-top-k-for-chatbot-responses/295542) and the [Cheat Sheet on Temperature and Top_p](https://community.openai.com/t/cheat-sheet-mastering-temperature-and-top-p-in-chatgpt-api/290953).\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f0296086-524a-414c-baa4-198d5a793a82",
   "metadata": {},
   "source": [
    "Lets start with a simple example using the default setting of temperature (0.8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "id": "4da894ce-46ce-44ab-af87-faca150d7e7e",
   "metadata": {},
   "outputs": [],
   "source": [
    "input_text= \"Write a short 4 line MAX poem about a dragon who discovers a hidden treasure in an unexpected place.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "id": "029c7938-f9ba-4ec5-9a86-c040ec4d9fe6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "assistant : In a village, blue and small,\n",
      "Smurfs stand united, short and tall.\n",
      "With love and laughter, they fill the day,\n",
      "In their enchanted, smurfy way.\n"
     ]
    }
   ],
   "source": [
    "response = client.chat.completions.create(\n",
    "    model = \"gpt-4-32k\",\n",
    "    messages = [{\"role\": \"user\", \"content\":input_text}],\n",
    "    temperature = 0.8\n",
    ")\n",
    "print(response.choices[0].message.role, \":\",response.choices[0].message.content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "id": "d77cbe76-68ef-4260-add8-758a1900879d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/json": {
       "choices": [
        {
         "content_filter_results": {
          "hate": {
           "filtered": false,
           "severity": "safe"
          },
          "self_harm": {
           "filtered": false,
           "severity": "safe"
          },
          "sexual": {
           "filtered": false,
           "severity": "safe"
          },
          "violence": {
           "filtered": false,
           "severity": "safe"
          }
         },
         "finish_reason": "stop",
         "index": 0,
         "logprobs": null,
         "message": {
          "content": "In a village, blue and small,\nSmurfs stand united, short and tall.\nWith love and laughter, they fill the day,\nIn their enchanted, smurfy way.",
          "function_call": null,
          "role": "assistant",
          "tool_calls": null
         }
        }
       ],
       "created": 1718670069,
       "id": "chatcmpl-9bGsvW5qShvl1u54QgD1X1G85UFjx",
       "model": "gpt-4-32k",
       "object": "chat.completion",
       "prompt_filter_results": [
        {
         "content_filter_results": {
          "hate": {
           "filtered": false,
           "severity": "safe"
          },
          "self_harm": {
           "filtered": false,
           "severity": "safe"
          },
          "sexual": {
           "filtered": false,
           "severity": "safe"
          },
          "violence": {
           "filtered": false,
           "severity": "safe"
          }
         },
         "prompt_index": 0
        }
       ],
       "system_fingerprint": null,
       "usage": {
        "completion_tokens": 37,
        "prompt_tokens": 19,
        "total_tokens": 56
       }
      },
      "text/plain": [
       "<IPython.core.display.JSON object>"
      ]
     },
     "execution_count": 152,
     "metadata": {
      "application/json": {
       "expanded": false,
       "root": "root"
      }
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "JSON(json.loads(response.model_dump_json()))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b9ae59c-2a99-41c5-bf96-5bee4dc8a95c",
   "metadata": {},
   "source": [
    "observation: each time we generate resonse we are getting different results."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b1bd7fb-f65a-43f9-900d-801a125a985f",
   "metadata": {},
   "source": [
    "Let's examine the effects of setting the temperature to zero.\n",
    "Observe how varying the temperature parameter influences the responses."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "id": "44092eaf-744e-4c5a-bd14-efa2ffc5c498",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------\n",
      "assistant : In the heart of a rose, a dragon did find,\n",
      "A treasure more precious, than gold ever mined.\n",
      "Not jewels or coins, but a love pure and kind,\n",
      "In the bloom of friendship, true wealth he'd unwind.\n",
      "----------\n",
      "assistant : In the heart of a rose, a dragon did find,\n",
      "A treasure more precious, than gold ever mined.\n",
      "Not jewels or coins, but a love pure and kind,\n",
      "In the bloom of a friendship, forever entwined.\n",
      "----------\n",
      "assistant : In the heart of a rose, a dragon did find,\n",
      "A treasure more precious, than gold ever mined.\n",
      "Not jewels or coins, but a love pure and kind,\n",
      "In the bloom of friendship, true wealth he'd unwind.\n",
      "----------\n",
      "assistant : In the heart of a rose, a dragon did find,\n",
      "A treasure more precious, than gold ever mined.\n",
      "Not jewels or coins, but a love pure and kind,\n",
      "In the bloom of friendship, true wealth he'd unwind.\n",
      "----------\n",
      "assistant : In the heart of a rose, a dragon did find,\n",
      "A treasure of love, the rarest kind.\n",
      "Not gold nor jewels, but something more,\n",
      "A feeling he'd never known before.\n"
     ]
    }
   ],
   "source": [
    "def create_response():\n",
    "    return client.chat.completions.create(\n",
    "    model = \"gpt-4-32k\",\n",
    "    temperature = 0,\n",
    "    messages = [{\"role\": \"user\", \"content\":input_text}]\n",
    "    )\n",
    "for _ in range(5):\n",
    "    response = create_response()\n",
    "    print(\"----------\")\n",
    "    print(response.choices[0].message.role, \":\",response.choices[0].message.content)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c633c4d1-fc4c-4372-a97a-d4139ecff27a",
   "metadata": {},
   "source": [
    "Observation: Setting the temperature parameter to zero results in the selection of the highest probability tokens, leading to concise and consistent responses.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d28c052a-cf20-4c46-8d7d-9d2ab730a903",
   "metadata": {},
   "source": [
    "### Using the 'max_tokens' Hyper-Parameter\n",
    "\n",
    "The `max_tokens` parameter in the OpenAI Chat Completion API sets the maximum number of tokens for the generated response.\n",
    "\n",
    "- **Purpose**: Controls the response length, manages costs, and ensures outputs stay within a specific limit.\n",
    "- **Usage**: Helps in generating concise responses and prevents excessively long outputs.\n",
    "- **Considerations**: Setting `max_tokens` too low can lead to incomplete responses with a stop reason of 'length'. It does not inherently make the model faster but helps control the output length and resource usage.\n",
    "\n",
    "> **Hint**: For more details, see the [OpenAI Developer Forum](https://community.openai.com/t/max-tokens-chat-completion-gpt4o/758066) and [OpenAI API Documentation](https://community.openai.com/t/clarification-for-max_tokens/758066).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "id": "28bfbb25-2b9a-4678-875b-95fcaa4e3168",
   "metadata": {},
   "outputs": [],
   "source": [
    "input_text= \"Write a short story about a dragon who discovers a hidden treasure in an unexpected place.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "id": "41816950-077d-4a2e-8a7e-b05050cd2590",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "assistant : Once upon a time, in the kingdom of Oregonia, lived a young dragon named Andros\n"
     ]
    }
   ],
   "source": [
    "response = client.chat.completions.create(\n",
    "    model = \"gpt-4-32k\",\n",
    "    max_tokens = 20,\n",
    "    messages = [{\"role\": \"user\", \"content\":input_text}]\n",
    ")\n",
    "print(response.choices[0].message.role, \":\",response.choices[0].message.content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "id": "cb1cdc59-f4d0-4ad7-8e43-812968ac26b0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/json": {
       "choices": [
        {
         "content_filter_results": {
          "hate": {
           "filtered": false,
           "severity": "safe"
          },
          "self_harm": {
           "filtered": false,
           "severity": "safe"
          },
          "sexual": {
           "filtered": false,
           "severity": "safe"
          },
          "violence": {
           "filtered": false,
           "severity": "safe"
          }
         },
         "finish_reason": "length",
         "index": 0,
         "logprobs": null,
         "message": {
          "content": "Once upon a time, in the kingdom of Oregonia, lived a young dragon named Andros",
          "function_call": null,
          "role": "assistant",
          "tool_calls": null
         }
        }
       ],
       "created": 1718665785,
       "id": "chatcmpl-9bFlpwc1Xv8FHPCU8NB1gnpGLyBKi",
       "model": "gpt-4-32k",
       "object": "chat.completion",
       "prompt_filter_results": [
        {
         "content_filter_results": {
          "hate": {
           "filtered": false,
           "severity": "safe"
          },
          "self_harm": {
           "filtered": false,
           "severity": "safe"
          },
          "sexual": {
           "filtered": false,
           "severity": "safe"
          },
          "violence": {
           "filtered": false,
           "severity": "safe"
          }
         },
         "prompt_index": 0
        }
       ],
       "system_fingerprint": null,
       "usage": {
        "completion_tokens": 20,
        "prompt_tokens": 24,
        "total_tokens": 44
       }
      },
      "text/plain": [
       "<IPython.core.display.JSON object>"
      ]
     },
     "execution_count": 126,
     "metadata": {
      "application/json": {
       "expanded": false,
       "root": "root"
      }
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "JSON(json.loads(response.model_dump_json()))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "62600ec8-e026-47e2-865a-0888055a7eb0",
   "metadata": {},
   "source": [
    "we can see that the generation stops at token 20 and it didnt finish the generation."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5df54633-c2a7-4ea6-be3a-3b6692f3c698",
   "metadata": {},
   "source": [
    "### Using the 'n' Hyper-Parameter\n",
    "\n",
    "The `n` parameter in the OpenAI Chat Completion API specifies the number of completions to generate for each prompt.\n",
    "\n",
    "- **Purpose**: Generates multiple response options, allowing you to choose the best one.\n",
    "- **Usage**: Useful for getting varied outputs for creative tasks or ensuring reliability in responses by selecting the most appropriate one from multiple options.\n",
    "- **Example**: Imagine you're drafting an email to a client and want to ensure the tone and content are just right. By setting `n` to 3, you can generate three different drafts of the email and choose the one that best fits the situation.\n",
    "\n",
    "> **Hint**: Setting `n` to a higher value will increase the number of completions generated, which can be useful for comparison and selection.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "id": "54393653-2f64-408d-a919-3d8460288a65",
   "metadata": {},
   "outputs": [],
   "source": [
    "input_text= \"tell me a short 4 lines joke\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "id": "8d8ce6ac-753d-43b8-9903-1a3d03f46256",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "assistant : Why don't scientists trust atoms?\n",
      "\n",
      "Because they make up everything!\n",
      "\n",
      "Isn't that funny?\n",
      "\n",
      "It's a pure chemistry joke!\n"
     ]
    }
   ],
   "source": [
    "response = client.chat.completions.create(\n",
    "    model = \"gpt-4-32k\",\n",
    "    n=5,\n",
    "    temperature = 1,\n",
    "    messages = [{\"role\": \"user\", \"content\":input_text}]\n",
    ")\n",
    "print(response.choices[0].message.role, \":\",response.choices[0].message.content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "id": "c99e33c9-9bc9-48d5-adfb-b17aa692d6f5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/json": {
       "choices": [
        {
         "content_filter_results": {
          "hate": {
           "filtered": false,
           "severity": "safe"
          },
          "self_harm": {
           "filtered": false,
           "severity": "safe"
          },
          "sexual": {
           "filtered": false,
           "severity": "safe"
          },
          "violence": {
           "filtered": false,
           "severity": "safe"
          }
         },
         "finish_reason": "stop",
         "index": 0,
         "logprobs": null,
         "message": {
          "content": "Why don't scientists trust atoms?\n\nBecause they make up everything!\n\nIsn't that funny?\n\nIt's a pure chemistry joke!",
          "function_call": null,
          "role": "assistant",
          "tool_calls": null
         }
        },
        {
         "content_filter_results": {
          "hate": {
           "filtered": false,
           "severity": "safe"
          },
          "self_harm": {
           "filtered": false,
           "severity": "safe"
          },
          "sexual": {
           "filtered": false,
           "severity": "safe"
          },
          "violence": {
           "filtered": false,
           "severity": "safe"
          }
         },
         "finish_reason": "stop",
         "index": 1,
         "logprobs": null,
         "message": {
          "content": "Why don’t scientists trust atoms?\n\nBecause they make up everything!\n\nThey even made up this joke. \n\nCan you atom-le believe it?",
          "function_call": null,
          "role": "assistant",
          "tool_calls": null
         }
        },
        {
         "content_filter_results": {
          "hate": {
           "filtered": false,
           "severity": "safe"
          },
          "self_harm": {
           "filtered": false,
           "severity": "safe"
          },
          "sexual": {
           "filtered": false,
           "severity": "safe"
          },
          "violence": {
           "filtered": false,
           "severity": "safe"
          }
         },
         "finish_reason": "stop",
         "index": 2,
         "logprobs": null,
         "message": {
          "content": "Why don't scientists trust atoms? \nBecause they make up everything! \nEven when they're split, \nThey make quite a lot of fuss!",
          "function_call": null,
          "role": "assistant",
          "tool_calls": null
         }
        },
        {
         "content_filter_results": {
          "hate": {
           "filtered": false,
           "severity": "safe"
          },
          "self_harm": {
           "filtered": false,
           "severity": "safe"
          },
          "sexual": {
           "filtered": false,
           "severity": "safe"
          },
          "violence": {
           "filtered": false,
           "severity": "safe"
          }
         },
         "finish_reason": "stop",
         "index": 3,
         "logprobs": null,
         "message": {
          "content": "Why don't scientists trust atoms?\n\nBecause they make up everything!\n\nThis joke is literally elemental.",
          "function_call": null,
          "role": "assistant",
          "tool_calls": null
         }
        },
        {
         "content_filter_results": {
          "hate": {
           "filtered": false,
           "severity": "safe"
          },
          "self_harm": {
           "filtered": false,
           "severity": "safe"
          },
          "sexual": {
           "filtered": false,
           "severity": "safe"
          },
          "violence": {
           "filtered": false,
           "severity": "safe"
          }
         },
         "finish_reason": "stop",
         "index": 4,
         "logprobs": null,
         "message": {
          "content": "Why don't scientists trust atoms?\n\nBecause they make up everything!\n\nIsn't that an electrifying joke?\n\nIt's pretty elementary!",
          "function_call": null,
          "role": "assistant",
          "tool_calls": null
         }
        }
       ],
       "created": 1718665796,
       "id": "chatcmpl-9bFm058Ka00qVWjNog7qN9mS1BCxt",
       "model": "gpt-4-32k",
       "object": "chat.completion",
       "prompt_filter_results": [
        {
         "content_filter_results": {
          "hate": {
           "filtered": false,
           "severity": "safe"
          },
          "self_harm": {
           "filtered": false,
           "severity": "safe"
          },
          "sexual": {
           "filtered": false,
           "severity": "safe"
          },
          "violence": {
           "filtered": false,
           "severity": "safe"
          }
         },
         "prompt_index": 0
        }
       ],
       "system_fingerprint": null,
       "usage": {
        "completion_tokens": 128,
        "prompt_tokens": 15,
        "total_tokens": 143
       }
      },
      "text/plain": [
       "<IPython.core.display.JSON object>"
      ]
     },
     "execution_count": 129,
     "metadata": {
      "application/json": {
       "expanded": false,
       "root": "root"
      }
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "JSON(json.loads(response.model_dump_json()))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf591cc3-fdb6-4fc2-90e3-48e862e5ba91",
   "metadata": {},
   "source": [
    "## Building Simple ChatCompletion with Memory\n",
    "\n",
    "Leveraging memory in ChatCompletion allows the model to retain context across multiple interactions, making conversations more coherent and contextually aware.\n",
    "\n",
    "### Power of Using Memory\n",
    "- **Enhanced Coherence**: By remembering previous interactions, the model can provide more relevant and contextually appropriate responses, leading to a more natural conversational flow.\n",
    "- **Context Retention**: Memory enables the model to build on past exchanges, improving the depth and continuity of the conversation.\n",
    "- **Personalization**: With memory, the model can recall user preferences and past interactions, creating a more personalized user experience.\n",
    "\n",
    "### Disadvantages\n",
    "- **Increased Complexity**: Managing memory and maintaining context can add complexity to the system, requiring careful handling of conversation history.\n",
    "- **Resource Intensive**: Storing and processing memory can consume more computational resources and increase response times.\n",
    "- **Potential for Errors**: If not managed correctly, memory can lead to inconsistencies or incorrect references to past interactions.\n",
    "\n",
    "Building a ChatCompletion with memory involves storing the conversation history and including it in each subsequent request to maintain context.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "id": "378c711c-9b6a-4e4f-aa87-7ced21327a4d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def append_to_history(role: str, content: str, memory: list[dict]):\n",
    "    if role == \"system\":\n",
    "        memory.append({\"role\": \"system\", \"content\":content})\n",
    "    if role == \"user\":\n",
    "        memory.append({\"role\": \"user\", \"content\":content})\n",
    "    if role == \"assistant\":\n",
    "        memory.append({\"role\": \"assistant\", \"content\":content})\n",
    "    return memory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "id": "dce9d7ff-ff30-4df2-bf47-c0d73827a89a",
   "metadata": {},
   "outputs": [],
   "source": [
    "memory = []\n",
    "\n",
    "system_message = \"You are a helpful assistant.\"\n",
    "input_text= \"hi!\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "id": "fc2862fa-c9cb-4e3a-94d8-ac1e30ab0695",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/json": [
       {
        "content": "You are a helpful assistant.",
        "role": "system"
       },
       {
        "content": "hi!",
        "role": "user"
       }
      ],
      "text/plain": [
       "<IPython.core.display.JSON object>"
      ]
     },
     "execution_count": 132,
     "metadata": {
      "application/json": {
       "expanded": false,
       "root": "root"
      }
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "memory = append_to_history(\"system\", system_message, memory)\n",
    "memory = append_to_history(\"user\", input_text, memory)\n",
    "JSON(memory)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "id": "a0659e54-6c21-42da-ba3b-e8e746e27021",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "assistant : Hello! How can I assist you today?\n",
      "\n"
     ]
    }
   ],
   "source": [
    "response = client.chat.completions.create(\n",
    "    model = \"gpt-4-32k\",\n",
    "    n=5,\n",
    "    temperature = 1,\n",
    "    messages = memory\n",
    ")\n",
    "print(response.choices[0].message.role, \":\",response.choices[0].message.content)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "47c5c6ec-485b-490e-97d7-dacbcf818f62",
   "metadata": {},
   "source": [
    "we will add the response to our conversation:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "id": "cce97831-f0da-406d-8c6d-906cb79d6c15",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/json": [
       {
        "content": "You are a helpful assistant.",
        "role": "system"
       },
       {
        "content": "hi!",
        "role": "user"
       },
       {
        "content": "Hello! How can I assist you today?\n",
        "role": "assistant"
       }
      ],
      "text/plain": [
       "<IPython.core.display.JSON object>"
      ]
     },
     "execution_count": 134,
     "metadata": {
      "application/json": {
       "expanded": false,
       "root": "root"
      }
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "memory = append_to_history(response.choices[0].message.role,response.choices[0].message.content, memory)\n",
    "JSON(memory)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a4977bdd-335e-43eb-8530-c52935eb5d52",
   "metadata": {},
   "source": [
    "### Create our chatbot with memory:\n",
    "\n",
    "Now, we are putting it all together into a chat-like application, simulating continuous interaction using a while loop.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "id": "580d6490-bfa7-41d3-b486-8b01b8a8f9fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_chat_response(memory):\n",
    "    return client.chat.completions.create(\n",
    "    model = \"gpt-35-32k\",\n",
    "    temperature = 0,\n",
    "    n=1,\n",
    "    messages = memory\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "id": "88d31878-ce9c-4e8b-9b95-3b5373daf839",
   "metadata": {},
   "outputs": [
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "user:  END\n"
     ]
    }
   ],
   "source": [
    "memory = []\n",
    "system_message = \"You are a helpful assistant.\"\n",
    "memory = append_to_history(\"system\", system_message, memory)\n",
    "while True:\n",
    "    user_input = input(\"user: \")\n",
    "    if user_input == \"END\":\n",
    "        break;\n",
    "    memory = append_to_history(\"user\", user_input, memory)\n",
    "    response = create_chat_response(memory)\n",
    "    memory = append_to_history(response.choices[0].message.role,response.choices[0].message.content, memory)\n",
    "    print(response.choices[0].message.role, \":\",response.choices[0].message.content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "id": "51b9ba89-6fab-40c2-8d5e-dd0e46149b30",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/json": [
       {
        "content": "You are a helpful assistant.",
        "role": "system"
       }
      ],
      "text/plain": [
       "<IPython.core.display.JSON object>"
      ]
     },
     "execution_count": 137,
     "metadata": {
      "application/json": {
       "expanded": false,
       "root": "root"
      }
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "JSON(memory)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c4d89e3d-f008-4637-8d85-80bcc93f493b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ab5fd29-3e6b-467e-b73e-d046a0c8e3a7",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "eeeb62c8-0e6c-4436-8b4a-cb7e2df1d0fd",
   "metadata": {},
   "source": [
    "# additional parameters for ChatCompletion:\n",
    "\n",
    "```\n",
    "return self._post(\n",
    "    664         \"/chat/completions\",\n",
    "    665         body=maybe_transform(\n",
    "    666             {\n",
    "    667                 \"messages\": messages,\n",
    "    668                 \"model\": model,\n",
    "    669                 \"frequency_penalty\": frequency_penalty,\n",
    "    670                 \"function_call\": function_call,\n",
    "    671                 \"functions\": functions,\n",
    "    672                 \"logit_bias\": logit_bias,\n",
    "    673                 \"logprobs\": logprobs,\n",
    "    674                 \"max_tokens\": max_tokens,\n",
    "    675                 \"n\": n,\n",
    "    676                 \"presence_penalty\": presence_penalty,\n",
    "    677                 \"response_format\": response_format,\n",
    "    678                 \"seed\": seed,\n",
    "    679                 \"stop\": stop,\n",
    "    680                 \"stream\": stream,\n",
    "    681                 \"temperature\": temperature,\n",
    "    682                 \"tool_choice\": tool_choice,\n",
    "    683                 \"tools\": tools,\n",
    "    684                 \"top_logprobs\": top_logprobs,\n",
    "    685                 \"top_p\": top_p,\n",
    "    686                 \"user\": user,\n",
    "    687             },\n",
    "    688             completion_create_params.CompletionCreateParams,\n",
    "    689         ),\n",
    "    690         options=make_request_options(\n",
    "    691             extra_headers=extra_headers, extra_query=extra_query, extra_body=extra_body, timeout=timeout\n",
    "    692         ),\n",
    "    693         cast_to=ChatCompletion,\n",
    "    694         stream=stream or False,\n",
    "    695         stream_cls=Stream[ChatCompletionChunk],\n",
    "    696     )\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7962164c-9d34-4ebb-9f90-a22a5f7ff01f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
